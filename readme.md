# Bifrost

Bifrost (/ˈbɪvrɒst/) is a tool for the evaulation and optimisation of DNN accelerators. The Bifrost interface bridges [Apache TVM](https://tvm.apache.org) (a deep learning compiler) with [STONNE](https://arxiv.org/pdf/2006.07137.pdf) (a simulator for DNN accelerators). Bifrost let's you run DNN models on simulated reconfigurable DNN accelerators.

The name is taken from Norse mythology, where Bifrost is the bridge between Midgard and Asgard. 

# Quickstart Guide

## Installation
Bifrost is a Python tool. You can install it using pip:
```
pip install git+https://github.com/axelstjerngren/level-4-project#"egg=bifrost&subdirectory=bifrost"
```
This will enable to you to use the latest version of Bifrost.

**N.B You need to have Apache TVM installed. You can find installation instructions [here](https://tvm.apache.org/docs/install/index.html).**

## How to use

Bifrost extends TVM to support STONNE as an external library. Most of the workflow is identical to the usual TVM workflow, but with extra fucntionality defined to configure a simulated accelerator and dataflow mapping. To get started, simply import tvm and bifrost.
``` python
import tvm
import bifrost
```
Importing TVM and Bifrost in this order is essential. Bifrost overrides the LLVM operators and adds new external ones which calls the STONNE library. When importing bifrost a new directory called ```bifrost_temp``` is created in your current working directory. This directory stores the temporary files generated by Bifrost when running, the cycle output in cycles.json, and the detailed STONNE infromation if ```architecture.print_stats``` is enabled. This directory can be deleted afer use (and you've made sure to get the data you need).

### Running a DNN model

When runnning a DNN model using Bifrost supported layers are automatically off-loaded to the simulated DNN accelerator and unsupported ones are executed on the TVM LLVM runtime backend. Currently, Bifrost supports NCHW conv2d and fully-connected (dense) layers. There is also support for NHWC conv2d layers, but the output is still unreliable at best. The simplest way to execute a DNN model is to use the built-in runners in Bifrost. Currently, PyTorch and ONNX models are supported.
``` python
 # Import the runner
from bifrost.runner.run import run_torch, run_onnx

# Get model and input
from alexnet import torch_model, input

# Run using Bifrost on STONNE. 
# If no architecture has been specified the default one will be used
output = run_torch(torch_model, input)
```

You can also run models from all deep learning libraries which are supported by TVM. Models from deep learning libraries other than PyTorch and ONNX can be used compiling them using TVM. [The TVM documentation contains gudies for PyTorch, Tensorflow, ONNX, MXNet, etc](https://tvm.apache.org/docs/tutorials/index.html#compile-deep-learning-models) models, just replace the target string with ```target = "llvm -libs=stone"```. The following example shows how to run a PyTorch model without using the Bifrost runner:

```python
# Import tvm and bifrost
import tvm
import bifrost

# Get model and input
from alexnet import torch_model, input

# Trace torch model and load into tvm
torch_model.eval()
trace = torch.jit.trace(torch_model, input).eval()
mod, params = relay.frontend.from_pytorch(trace, [("trace", input.shape)])

# Use STONNE as an external library
target = "llvm -libs=stonne"

# Run the model
lib = relay.build(mod, target=target, params=params)
ctx = tvm.context(target, 0)
module = runtime.GraphModule(lib["default"](ctx))
module.set_input("trace", input)
output = module.run()
```

### Configuring the simulated architecture
The general structure of a simulated DNN accelerator comprises of a spatial array of processing elements (PEs). Each PE contains a multiply-accumulate unit (MAC). The PEs receive their inputs and weights from the distribution network and write outputs back to the buffer using the reduction network:
![STONNE structure](https://drive.google.com/uc?export=view&id=15K-3DWHoYzPDFAWrTtioaOmvrL_8bMx0)

|Setting|Description|Options|
| --- | --- | --- |
| controller_type |The simulated architecture such as MAERI, SIGMA, and the TPU|"MAERI_DENSE_WORKLOAD", "SIGMA_SPARSE_GEMM", or "TPU_OS_DENSE"|
| ms_network_type |Defines the mulitplier type. Flexible architectures use LINEAR while rigid ones like the TPU must use OS_MESH |"LINEAR","OS_MESH"|
| ms_size | The number of multipliers (PEs) in the architecture, used if ms_network_type is LINEAR (not OS_MESH.)|Power of two and >= 8 |
| ms_row  | If ms_network_type is OS_MESH the PEs are organised into rows and columns,  |Power of two and >= 8|
| ms_col  | If ms_network_type is OS_MESH the PEs are organised into rows and columns,  |Power of two and >= 8|
| reduce_network_type |The type of reduction network|"ASNETWORK","FENETWORK","TEMPORALRN"|
| dn_bw | Number of read ports (distribution network)|Power of two and >= 8 |
| rn_bw | Number of write ports (reduction network)|Power of two and >= 8 |
| sparsity_ratio | The sparsity of the architecture|[0,10]|
| accumulation_buffer_enabled |Accumulation buffer, required to be enabled for rigid architectures like the TPU  |True or False|

To use the TPU the following settings are required:
|Setting|Selected Option|
| -- | -- |
|controller_type |TPU_OS_DENSE|
|ms_network_type|OS_MESH|
|ms_row|Integer which is a power two and >= 8|
|ms_col|Integer which is a power two and >= 8|
|accumulation_buffer_enabled|True|

The simulated architecture is configured through the architecture module:
``` python
# Import the architecture module 
from bifrost.stonne.simulator import architecture

# Configure the simulated architecture
architecture.ms_size = 128
architecture.rn_bw = 64
architecture.dn_bw = 64
architecture.controller_type = "SIGMA_SPARSE_GEMM"
architecture.sparsity_ratio = 0

# Create the config file which is used by STONNE
architecture.create_config_file()
```
If the architecture is not configured the following configuration is used:
|Option|Default|
| -- | -- |
|ms_size|16|
|reduce_network_type|ASNETWORK|
|ms_network_type|LINEAR|
|dn_bw|8|
|rn_bw|8|
|controller_type|MAERI_DENSE_WORKLOAD|
|accumulation_buffer_enabled|True|

By default STONNE will not create any output files during execution. This setting can be enabled by setting ```architecture.print_stats = True```

### Configure dataflow mapping




```
architecture.load_mapping(
  conv = [],
  fc =[],
)
```





### Tuning 
When tuning the mapping or the hardware for a DNN, we first need to set 
``` python
from bifrost.stonne.simulator import architecture
# Set the tuning to true
architecture.tune = True

```
You need to access the tuning module to create the tuning space
``` python
architecture.tuner
```

|Setting|Variable|Options|
| -- | -- | -- |
|tune_convolutions_tile|conv_num|Integer|


``` python
from bifrost.stonne.simulator import config_simulator, architecture

# Set the architecture
architecture.ms_size = 128
architecture.dn_bw=64
architecture.rn_bw=64
architecture.controller_type = "MAERI_DENSE_WORKLOAD"
architecture.create_config_file()

# Enable tuning
architecture.tune = True

# Set tuning to be based on partial sums instead of cycles
architecture.tuner.tune_psums = True

# Tune mapping for both convolutions and fully connected layers
#architecture.tuner.tune_convolutions_tile = True
#architecture.tuner.tune_fc_tile = True

# Set the range to test
#architecture.tuner.conv_num = 20
#architecture.tuner.fc_num = 20

# For AutoTVM to work, a __name__ == "__main__" block is required. When tuning, 
# each unique combination of parameters represents a point in the tuning space.
# For each point, a new TVM instance is launched. All settings which you want
# to be constant across all instances have to be set outside of this block.
if __name__ == "__main__":
  # Standard imports
  import tvm
  import bifrost 

  from bifrost.tuner.stonne_builder import StonneLocalBuilder, StonneLocalRunner
  from tvm.autotvm.tuner import XGBTuner, GATuner, RandomTuner, GridSearchTuner
  
  from alexnet import alex_model as torch_model
  from alexnet import input_batch

  torch_model.eval()
  trace = torch.jit.trace(torch_model, input_batch).eval()
    
  mod, params = relay.frontend.from_pytorch(trace, [("trace", input_batch.shape)])


  target = "llvm --libs=stonne"
  
  # The output from the tuning process is placed in a separate file.
  # It is recommended to use bifrost_temp to make sure that all files arwe kept in the
  # same place.
  log_file = "bifrost_temp/alexnet.log"
  
  # Set the tuning options
  tuning_options = {
        "log_filename": log_file,
        "tuner": "xgb",
        "early_stopping": None,
        "measure_option": autotvm.measure_option(
            builder=StonneLocalBuilder(),
            runner=StonneLocalRunner(),
        ),
    }


  tasks = autotvm.task.extract_from_program(
            mod, 
            target=target, 
            params=params, 
            ops=(relay.op.get("nn.conv2d"),)
  )
  for i, task in enumerate(tasks):
    prefix = "[Task %2d/%2d] " % (i + 1, len(tasks))
    
    # Create the tuner
    tuner_obj = XGBTuner(task, loss_type="rank")

    # do tuning
    n_trial = len(task.config_space)/10


    tuner_obj.tune(
       n_trial=n_trial,
       measure_option=measure_option,
       callbacks=[
         autotvm.callback.progress_bar(100, prefix=prefix),
         autotvm.callback.log_to_file(log_filename),
    ],

```




# Advanced Instructions 
## Build from source

Install Apache TVM using the installation instructions [here](https://tvm.apache.org/docs/install/index.html).

Clone the project and cd into bifrost
```
git clone https://github.com/axelstjerngren/level-4-project
cd level-4-project/bifrost
```
You can now install it by running setup.py:
```
python setup.py install 
```
You can now use Bifrost.

Alternatively, if you are going to make modifications to Bifrost then export it to PYTHONPATH to tell python where to find the library. This way your changes will immeditaly be reflected and there is no need to call setup.py again.
```
export BIFROST=/path/to/level-4-project/bifrost/
export PYTHONPATH=$BIFROST/python:${PYTHONPATH}
```

## Modifying the C++ code 
All of the C++ files can be found in under:
```
level-4-project
|___bifrost
|    |__src
|    |   |__include
|    |   |     |__cost.h
|    |   |
|    |   |__conv_forward.cpp
|    |   |__cost.cpp
|    |   |__json.cpp
|    |   |__etc...
|    |__Makefile
```

Any new .cpp files will be automatically found by the Makefile as long as they are created within the /src folder. Before you compile the code you need STONNE, mRNA and TVM as enviroment variables (see next section) You can the compile your new code with the following commands:
```
cd bifrost
make -j
```

### C++ depdencies 
To change the C code you need to clone the STONNE, mRNA and TVM repositories:
```
git clone https://github.com/axelstjerngren/stonne
git clone https://github.com/axelstjerngren/mrna
git clone https://github.com/apache/tvm
```
Keeping these three in the same folder will be useful.
Before you can run **make** you need to export two environment variables:
```
export TVM_ROOT    = path_to_tvm/tvm
export STONNE_ROOT = path_to_stonne/stonne
export MRNA_ROOT   = path_to_stonne/stonne
```
The C++ should now compile correctly when you run **make** inside of the level-4-project/bifrost directory.

## Dependecies


Python >=3.8
* Apache TVM |  | 
* STONNE |A cycle-accurate simulator for reconfigurable DNN accelerators written in C++, a forked version is required for Bifrost| 
]* JSONCPP |A library to read/write JSON files for C++| https://github.com/open-source-parsers/jsoncpp

STONNE
TVM
MRNA

**N.B** If you have TVM installed, you only need to run the pip command above. The C++ dependencies come pre-packaged together with Bifrost

## Run the tests
Bifrost includes a test suite to ensure the correctness of the supported operations. This will run all implemented layers (conv2d and dense) on STONNE and compare the output against the TVM LLVM implementation for correctness. The MAERI, SIGMA, and TPU architectures will be tested. You can run the tests using the following commands:
```
cd bifrost
python setup.py
```
Tested on macOS Big Sur (11.1) and Manjaro 20.2.1 

### Architecture

![Bifrost diagram](https://drive.google.com/uc?export=view&id=1YNvC9asfmgpLy4Pl6nDMuHG23A1TneEj)




